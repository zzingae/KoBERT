{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36264bitvenvvenv128625fee99c462192326d997787e3fe",
   "display_name": "Python 3.6.2 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # 상위 폴더의 model.py를 import 하기 위해 path 추가\n",
    "from model import make_model\n",
    "import pandas as pd\n",
    "from kobert.utils import get_tokenizer\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "\n",
    "path='../data/ChatBotData.csv'\n",
    "\n",
    "df = pd.read_csv(path, header=0, encoding='utf-8')\n",
    "_, vocab = make_model(1)\n",
    "sp = SentencepieceTokenizer(get_tokenizer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Q</th>\n      <th>A</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12시 땡!</td>\n      <td>하루가 또 가네요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1지망 학교 떨어졌어</td>\n      <td>위로해 드립니다.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3박4일 놀러가고 싶다</td>\n      <td>여행은 언제나 좋죠.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3박4일 정도 놀러가고 싶다</td>\n      <td>여행은 언제나 좋죠.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PPL 심하네</td>\n      <td>눈살이 찌푸려지죠.</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ids, a_ids = [], []\n",
    "for i in range(len(df)):\n",
    "    question = df.loc[i, 'Q']\n",
    "    answer = df.loc[i, 'A']\n",
    "\n",
    "    q_ids.append([vocab.token_to_idx[tok] for tok in sp(question)])\n",
    "    a_ids.append([vocab.token_to_idx[tok] for tok in sp(answer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "vocab.idx_to_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "q_count = Counter()\n",
    "for i in range(len(q_ids)):\n",
    "    q_count += Counter(q_ids[i])\n",
    "\n",
    "a_count = Counter()\n",
    "for i in range(len(a_ids)):\n",
    "    a_count += Counter(a_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sorted = sorted(q_count.items(), key=lambda x: x[1], reverse=True)\n",
    "a_sorted = sorted(a_count.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Among questions\ntop 0 frequent word: ▁ : 3852\ntop 1 frequent word: . : 2238\ntop 2 frequent word: 이 : 1817\ntop 3 frequent word: 가 : 1792\ntop 4 frequent word: ? : 1702\ntop 5 frequent word: 어 : 1358\ntop 6 frequent word: 까 : 1194\ntop 7 frequent word: 지 : 1140\ntop 8 frequent word: 네 : 876\ntop 9 frequent word: ▁좋아 : 847\nUNK count: 249\ntotal words count: 93131, average length: 7.877103949928106\n"
     ]
    }
   ],
   "source": [
    "print('Among questions')\n",
    "total_count=0\n",
    "for i,(idx,count) in enumerate(q_sorted):\n",
    "    if i < 10: \n",
    "        print('top {} frequent word: {} : {}'.format(i,vocab.idx_to_token[idx],count))\n",
    "    if idx==vocab.token_to_idx['UNK']: \n",
    "        print('UNK count: {}'.format(count))\n",
    "    total_count += count\n",
    "print('total words count: {}, average length: {}'.format(total_count,total_count/len(q_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Among answers\ntop 0 frequent word: .\ntop 1 frequent word: 요\ntop 2 frequent word: 세요\ntop 3 frequent word: ▁\ntop 4 frequent word: 이\ntop 5 frequent word: 보\ntop 6 frequent word: 가\ntop 7 frequent word: ▁거\ntop 8 frequent word: 을\ntop 9 frequent word: 예\nUNK count: 88\ntotal words count: 109201, average length: 9.236319039160957\n"
     ]
    }
   ],
   "source": [
    "print('Among answers')\n",
    "total_count = 0\n",
    "for i,(idx,count) in enumerate(a_sorted):\n",
    "    if i < 10: \n",
    "        print('top {} frequent word: {}'.format(i,vocab.idx_to_token[idx]))\n",
    "    if idx==vocab.token_to_idx['UNK']: \n",
    "        print('UNK count: {}'.format(count))\n",
    "    total_count += count\n",
    "print('total words count: {}, average length: {}'.format(total_count,total_count/len(a_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sentences, a_sentences = [], []\n",
    "for i in range(len(df)):\n",
    "    question = df.loc[i, 'Q']\n",
    "    answer = df.loc[i, 'A']\n",
    "\n",
    "    q_sentences.append(question)\n",
    "    a_sentences.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.']"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "a_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@richardcsaky/neural-chatbots-are-dumb-65b6b40e9bd4\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distribution(data):\n",
    "  # Read data and store it in a list.\n",
    "#   with open(data) as file:\n",
    "#     clean_sentences = [' '.join(s.split()) for s in file]\n",
    "  clean_sentences = data\n",
    "  # Build a dict of unique sentences storing indexes to the original list.\n",
    "  sentence_dict = {}\n",
    "  for i, sentence in enumerate(clean_sentences):\n",
    "    sentence_dict[sentence] = sentence_dict.get(sentence, [])\n",
    "    sentence_dict[sentence].append(i)\n",
    "    \n",
    "  return sentence_dict, clean_sentences\n",
    "\n",
    "source_distro, sources = build_distribution(q_sentences)\n",
    "target_distro, targets = build_distribution(a_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "source top 5 duplicated sentences\n[('외로워', [3456, 3457, 7261, 7262]), ('너무 힘들다', [980, 5944, 5945]), ('올해 왜 이러지', [3387, 3388, 3389]), ('사랑을 했다', [9897, 9898, 9899]), ('결혼이나 하지 왜 자꾸 나한테 화 내냐구!', [152, 5527])]\n\ntarget top 5 duplicated sentences\n[('맛있게 드세요.', [45, 49, 50, 1263, 1267, 1270, 1271, 1390, 1391, 1888, 1889, 1972, 1976, 2245, 2246, 2678, 2680, 2682, 3371, 4436, 4437, 4581]), ('제가 있잖아요.', [626, 632, 824, 825, 1461, 1552, 3598, 3599, 4014, 4015, 4587, 4589, 4703, 4704, 6811, 8235, 8830]), ('조심하세요.', [409, 411, 989, 1699, 1700, 1761, 1762, 2171, 2172, 2243, 3820, 3924, 4891, 5062]), ('감기 조심하세요.', [1326, 1327, 1899, 1903, 2398, 2846, 3079, 3080, 3300, 4511, 4512, 4754, 5826, 6551]), ('잘할 수 있을 거예요.', [1190, 1191, 1948, 1949, 3822, 4980, 4981, 5221, 5228, 5230, 6050, 6512])]\n"
     ]
    }
   ],
   "source": [
    "print('source top 5 duplicated sentences')\n",
    "print(sorted(source_distro.items(),key=lambda x: len(x[1]),reverse=True)[:5])\n",
    "print('')\n",
    "print('target top 5 duplicated sentences')\n",
    "print(sorted(target_distro.items(),key=lambda x: len(x[1]),reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropies(distro, pairs):\n",
    "  entropies = {}\n",
    "\n",
    "  for sentence, indices in distro.items():\n",
    "    # Build a distribution for the current sentence, based on the targets.\n",
    "    # 어떤 source sentence 가 나온 위치들에서의 target sentence들의 Counter를 구함.\n",
    "    distribution = Counter([pairs[i] for i in indices])\n",
    "    num_elements = len(indices)\n",
    "\n",
    "    # Calculate entropy.\n",
    "    entropy = 0\n",
    "    for frequency in distribution.values():\n",
    "      probability = frequency / num_elements\n",
    "      entropy += probability * math.log(probability, 2)\n",
    "    \n",
    "    entropies[sentence] = -entropy\n",
    "  return entropies\n",
    "\n",
    "source_entropies = get_entropies(source_distro, targets)\n",
    "target_entropies = get_entropies(target_distro, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "source 5 highest entropy sentences\n[('올해 왜 이러지', 1.584962500721156), ('사랑을 했다', 1.584962500721156), ('외로워', 1.5), ('고양이 키우고 싶어', 1.0), ('공시 준비 힘들어', 1.0)]\n\ntarget 5 highest entropy sentences\n[('맛있게 드세요.', 4.459431618637295), ('제가 있잖아요.', 4.08746284125034), ('조심하세요.', 3.8073549220576055), ('감기 조심하세요.', 3.8073549220576055), ('잘할 수 있을 거예요.', 3.584962500721156)]\n"
     ]
    }
   ],
   "source": [
    "print('source 5 highest entropy sentences')\n",
    "print(sorted(source_entropies.items(),key=lambda x: x[1],reverse=True)[:5])\n",
    "print('')\n",
    "print('target 5 highest entropy sentences')\n",
    "print(sorted(target_entropies.items(),key=lambda x: x[1],reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0 # 2번 나온것들을 사용하지 않음. (0.5 * log_2(0.5))*2\n",
    "source_refine=[]\n",
    "target_refine=[]\n",
    "for source, target in zip(sources, targets):\n",
    "  # Here we can put whatever combination of the entropy thresholds.\n",
    "  if source_entropies[source] < threshold and target_entropies[target] < threshold:\n",
    "    source_refine.append(source)\n",
    "    target_refine.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "# dummy label is added for the same format with original csv\n",
    "refined_data = {'Q': source_refine, 'A': target_refine, 'label': [0]*len(source_refine)}\n",
    "data_df = DataFrame(refined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                             Q                                            A  \\\n",
       "0                         겁난다                                     용기 내보세요.   \n",
       "1               공책 필기 나만 힘들어?                             성향 차이가 좀 있기는 하죠.   \n",
       "2                기다리라고 말 못하겠어                              상대방의 선택에 맡겨보세요.   \n",
       "3                   기분이 그지 같아                                신나는 음악 들어보세요.   \n",
       "4                     기분이 더러워                                경쾌한 음악 들어보세요.   \n",
       "...                       ...                                          ...   \n",
       "4757   회사에서 어떤 사람이랑 자꾸 눈 마추쳐.                      눈 마주치는 게 우연인지 잘 살펴 보세요.   \n",
       "4758      회식 중이라고 하는데 연락이 안돼.  정신 없이 바쁠지도 몰라요. 조금만 더 기다려보고 물어보는게 좋을 것 같아요.   \n",
       "4759            후회 없이 사랑하고 싶어                                진심으로 다가가 보세요.   \n",
       "4760              흑기사 해주는 짝남.                                       설렜겠어요.   \n",
       "4761  힘든 연애 좋은 연애라는게 무슨 차이일까?                     잘 헤어질 수 있는 사이 여부인 거 같아요.   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "4757      0  \n",
       "4758      0  \n",
       "4759      0  \n",
       "4760      0  \n",
       "4761      0  \n",
       "\n",
       "[4762 rows x 3 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "data_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('../data/ChatBotData_refined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}