{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import make_model\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from utils import greedy_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_layers = 6\n",
    "model_path = './output/step_26001.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "using cached model\nusing cached model\nusing cached model\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, vocab = make_model(num_decoder_layers)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)['model'])\n",
    "model.eval()\n",
    "sp  = SentencepieceTokenizer(get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "question: ['▁사랑', '해', '?']\nanswer: ▁당신이요.\n"
    }
   ],
   "source": [
    "question = '사랑해?'\n",
    "max_len = 20\n",
    "\n",
    "tokens = sp(question)\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "token_ids = [vocab.token_to_idx[tok] for tok in tokens]\n",
    "# unsqueeze(0) for Batch position (zzingae)\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "# attention score: [Batch, Head, tgt_length, src_length] in src_attn (zzingae)\n",
    "# unsqueeze(1) for tgt_length position (zzingae)\n",
    "attn_mask = (token_ids != vocab.token_to_idx['[PAD]']).unsqueeze(1).long()\n",
    "\n",
    "answer = greedy_decode(model, token_ids, attn_mask, max_len, vocab)\n",
    "\n",
    "with open('QnA_examples.txt','a',encoding='utf-8') as txt:\n",
    "    Q = 'question: {}'.format(sp(question))\n",
    "    A = 'answer: '+''.join([vocab.idx_to_token[idx] for idx in answer[0,1:]])\n",
    "    txt.write(Q+'\\n'+A+'\\n')\n",
    "    print(Q)\n",
    "    print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}