{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36264bitvenvvenv128625fee99c462192326d997787e3fe",
   "display_name": "Python 3.6.2 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "I0909 15:11:14.576323 140734738472384 file_utils.py:39] PyTorch version 1.6.0 available.\n"
    }
   ],
   "source": [
    "import torch\n",
    "from model import make_model\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from utils import greedy_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_layers = 3\n",
    "model_path = './chatbot-ckpt/step_60000.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "I0909 14:30:43.259646 140734987816384 configuration_utils.py:300] Model config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 8002\n}\n\nusing cached model\nusing cached model\nusing cached model\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, vocab = make_model(num_decoder_layers)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)['model'])\n",
    "model.eval()\n",
    "sp  = SentencepieceTokenizer(get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "question: ['▁사랑', '했던', '▁그', '▁사람의', '▁다른', '▁모습이', '▁보', '인', '거', '예', '요', '??']\nanswer: ▁마음을▁이어보고▁싶어하는▁사람이▁만들었어요.\n"
    }
   ],
   "source": [
    "question = '사랑했던 그 사람의 다른 모습이 보인거예요??'\n",
    "max_len = 20\n",
    "\n",
    "tokens = sp(question)\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "token_ids = [vocab.token_to_idx[tok] for tok in tokens]\n",
    "# unsqueeze(0) for Batch position (zzingae)\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "# attention score: [Batch, Head, tgt_length, src_length] in src_attn (zzingae)\n",
    "# unsqueeze(1) for tgt_length position (zzingae)\n",
    "attn_mask = (token_ids != vocab.token_to_idx['[PAD]']).unsqueeze(1).long()\n",
    "\n",
    "answer = greedy_decode(model, token_ids, attn_mask, max_len, vocab)\n",
    "\n",
    "print('question: {}'.format(sp(question)))\n",
    "print('answer: '+''.join([vocab.idx_to_token[idx] for idx in answer[0,1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}